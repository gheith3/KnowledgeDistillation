{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Knowledge Distillation for EfficientNetV2 on CIFAR-100 (SOTA)\n",
        "\n",
        "This notebook implements **State-of-the-Art** knowledge distillation with:\n",
        "\n",
        "- **Decoupled Knowledge Distillation (DKD)** - Separates target and non-target class knowledge\n",
        "- **CutMix + Mixup** - Advanced data augmentation\n",
        "- **Google Drive integration** for persistent checkpoint storage\n",
        "\n",
        "---\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "### Google Colab Setup:\n",
        "\n",
        "1. **Enable GPU:** Runtime -> Change runtime type -> **GPU (T4)**\n",
        "2. **Run Cell 1** to mount Google Drive\n",
        "3. **Training will auto-resume** from the last checkpoint if interrupted\n",
        "\n",
        "### Expected Results:\n",
        "\n",
        "- **Teacher (EfficientNetV2-L):** ~68-70% accuracy\n",
        "- **Distilled Student (DKD):** ~69-72% accuracy\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Setup and Imports\n",
        "!pip install thop -q\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import efficientnet_v2_s, efficientnet_v2_l, EfficientNet_V2_S_Weights, EfficientNet_V2_L_Weights\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import json\n",
        "import random\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Setup Directories\n",
        "DRIVE_ROOT = '/content/drive/MyDrive/KnowledgeDistillation_SOTA'\n",
        "MODEL_DIR = f'{DRIVE_ROOT}/models'\n",
        "DATA_DIR = f'{DRIVE_ROOT}/data'\n",
        "CHECKPOINT_DIR = f'{DRIVE_ROOT}/checkpoints'\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Directories ready at: {DRIVE_ROOT}\")\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Experiment Configuration (SOTA)\n",
        "# ==========================================\n",
        "# HYPERPARAMETERS\n",
        "# ==========================================\n",
        "NUM_EPOCHS = 200            # Increased for proper convergence with Distillation\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.001       # Initial LR\n",
        "WEIGHT_DECAY = 0.05         # Strong regularization\n",
        "PATIENCE = 30               # Increased patience for early stopping\n",
        "\n",
        "# Distillation Params (DKD)\n",
        "DKD_ALPHA = 1.0             # Weight for Target Knowledge\n",
        "DKD_BETA = 8.0              # Weight for Non-Target Knowledge (Crucial)\n",
        "TEMPERATURE = 4.0           # Softmax Temperature\n",
        "\n",
        "# Augmentation Params\n",
        "MIXUP_ALPHA = 0.8\n",
        "CUTMIX_ALPHA = 1.0\n",
        "CHECKPOINT_FREQUENCY = 20\n",
        "NUM_CLASSES = 100\n",
        "\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"CONFIG: Epochs={NUM_EPOCHS} | Batch={BATCH_SIZE} | Temp={TEMPERATURE}\")\n",
        "print(f\"DKD: Alpha={DKD_ALPHA}, Beta={DKD_BETA}\")\n",
        "print(f\"{'='*50}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Data Loading\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # Note: CutMix/Mixup are applied in the training loop\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root=DATA_DIR, train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, \n",
        "                                          num_workers=2, pin_memory=True, drop_last=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root=DATA_DIR, train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, \n",
        "                                         num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Data loaded: {len(trainset)} Training, {len(testset)} Test images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Helper Functions (DKD Loss, CutMix, Utils)\n",
        "\n",
        "# ==========================================\n",
        "# 1. Decoupled Knowledge Distillation (DKD)\n",
        "# Paper: https://arxiv.org/abs/2203.08679\n",
        "# ==========================================\n",
        "def dkd_loss(student_logits, teacher_logits, target, alpha=1.0, beta=8.0, temp=4.0):\n",
        "    gt_mask = _get_gt_mask(student_logits, target)\n",
        "    other_mask = _get_other_mask(student_logits, target)\n",
        "    \n",
        "    pred_student = F.softmax(student_logits / temp, dim=1)\n",
        "    pred_teacher = F.softmax(teacher_logits / temp, dim=1)\n",
        "    \n",
        "    # Target Class Knowledge Distillation (TCKD)\n",
        "    pred_student_tckd = _cat_mask(pred_student, gt_mask, other_mask)\n",
        "    pred_teacher_tckd = _cat_mask(pred_teacher, gt_mask, other_mask)\n",
        "    log_pred_student_tckd = torch.log(pred_student_tckd + 1e-8)\n",
        "    \n",
        "    tckd_loss = (\n",
        "        F.kl_div(log_pred_student_tckd, pred_teacher_tckd, reduction='batchmean')\n",
        "        * (temp**2)\n",
        "    )\n",
        "    \n",
        "    # Non-Target Class Knowledge Distillation (NCKD)\n",
        "    pred_teacher_part2 = F.softmax(\n",
        "        teacher_logits / temp - 1000.0 * gt_mask, dim=1\n",
        "    )\n",
        "    log_pred_student_part2 = F.log_softmax(\n",
        "        student_logits / temp - 1000.0 * gt_mask, dim=1\n",
        "    )\n",
        "    \n",
        "    nckd_loss = (\n",
        "        F.kl_div(log_pred_student_part2, pred_teacher_part2, reduction='batchmean')\n",
        "        * (temp**2)\n",
        "    )\n",
        "    \n",
        "    return alpha * tckd_loss + beta * nckd_loss\n",
        "\n",
        "def _get_gt_mask(logits, target):\n",
        "    target = target.reshape(-1)\n",
        "    mask = torch.zeros_like(logits).scatter_(1, target.unsqueeze(1), 1).bool()\n",
        "    return mask\n",
        "\n",
        "def _get_other_mask(logits, target):\n",
        "    target = target.reshape(-1)\n",
        "    mask = torch.ones_like(logits).scatter_(1, target.unsqueeze(1), 0).bool()\n",
        "    return mask\n",
        "\n",
        "def _cat_mask(t, mask1, mask2):\n",
        "    t1 = (t * mask1).sum(dim=1, keepdims=True)\n",
        "    t2 = (t * mask2).sum(1, keepdims=True)\n",
        "    rt = torch.cat([t1, t2], dim=1)\n",
        "    return rt\n",
        "\n",
        "# ==========================================\n",
        "# 2. Augmentations: Mixup & CutMix\n",
        "# ==========================================\n",
        "def mixup_data(x, y, alpha=1.0):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "def cutmix_data(x, y, alpha=1.0):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(device)\n",
        "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
        "    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n",
        "    y_a, y_b = y, y[index]\n",
        "    return x, y_a, y_b, lam\n",
        "\n",
        "# ==========================================\n",
        "# 3. Utilities (Save/Load/Evaluate)\n",
        "# ==========================================\n",
        "def evaluate_model_with_loss(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    acc = 100 * correct / total\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    return acc, avg_loss\n",
        "\n",
        "def save_checkpoint(model, optimizer, scheduler, epoch, best_acc, history, model_name, epochs_no_improve):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'best_acc': best_acc,\n",
        "        'history': history,\n",
        "        'epochs_no_improve': epochs_no_improve\n",
        "    }\n",
        "    path = f\"{CHECKPOINT_DIR}/{model_name}_epoch{epoch+1}.pth\"\n",
        "    torch.save(checkpoint, path)\n",
        "    print(f\"  Checkpoint saved: {path}\")\n",
        "    \n",
        "def load_checkpoint(model, optimizer, scheduler, model_name):\n",
        "    checkpoints = sorted(glob.glob(f\"{CHECKPOINT_DIR}/{model_name}_epoch*.pth\"))\n",
        "    if not checkpoints:\n",
        "        return None\n",
        "    latest = checkpoints[-1]\n",
        "    print(f\"  Loading checkpoint: {latest}\")\n",
        "    checkpoint = torch.load(latest)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    return checkpoint\n",
        "\n",
        "def cleanup_old_checkpoints(model_name, keep=3):\n",
        "    checkpoints = sorted(glob.glob(f\"{CHECKPOINT_DIR}/{model_name}_epoch*.pth\"))\n",
        "    if len(checkpoints) > keep:\n",
        "        for chk in checkpoints[:-keep]:\n",
        "            os.remove(chk)\n",
        "            print(f\"  Cleaned up: {os.path.basename(chk)}\")\n",
        "\n",
        "print(\"Helper functions loaded (DKD, CutMix, Mixup)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Optimized Training Loop\n",
        "def train_model_optimized(model, dataloader, optimizer, scheduler, num_epochs, model_name, \n",
        "                         teacher_model=None, dkd_alpha=1.0, dkd_beta=8.0, temp=4.0, \n",
        "                         patience=30, grad_clip=1.0):\n",
        "    \n",
        "    # 1. Load Checkpoint\n",
        "    checkpoint = load_checkpoint(model, optimizer, scheduler, model_name)\n",
        "    if checkpoint:\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        best_acc = checkpoint['best_acc']\n",
        "        history = checkpoint['history']\n",
        "        epochs_no_improve = checkpoint['epochs_no_improve']\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        print(f\"  Resuming from epoch {start_epoch}, Best Acc: {best_acc:.2f}%\")\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "        best_acc = 0.0\n",
        "        history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
        "        epochs_no_improve = 0\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        print(f\"  Starting fresh training...\")\n",
        "\n",
        "    # 2. Setup\n",
        "    scaler = torch.amp.GradScaler('cuda', enabled=torch.cuda.is_available())\n",
        "    val_criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    if teacher_model:\n",
        "        teacher_model.eval()\n",
        "\n",
        "    # 3. Training Loop\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        \n",
        "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        \n",
        "        for inputs, labels in loop:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            # Randomly choose Mixup (50%) or CutMix (50%)\n",
        "            use_cutmix = np.random.rand() > 0.5\n",
        "            if use_cutmix:\n",
        "                inputs_aug, labels_a, labels_b, lam = cutmix_data(inputs.clone(), labels, alpha=CUTMIX_ALPHA)\n",
        "            else:\n",
        "                inputs_aug, labels_a, labels_b, lam = mixup_data(inputs, labels, alpha=MIXUP_ALPHA)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            with torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n",
        "                # Teacher Forward\n",
        "                if teacher_model:\n",
        "                    with torch.no_grad():\n",
        "                        teacher_outputs = teacher_model(inputs_aug)\n",
        "                \n",
        "                # Student Forward\n",
        "                student_outputs = model(inputs_aug)\n",
        "                \n",
        "                # Loss Calculation\n",
        "                if teacher_model:\n",
        "                    # DKD Loss (Calculated for both mixed labels)\n",
        "                    loss_a = dkd_loss(student_outputs, teacher_outputs, labels_a, dkd_alpha, dkd_beta, temp)\n",
        "                    loss_b = dkd_loss(student_outputs, teacher_outputs, labels_b, dkd_alpha, dkd_beta, temp)\n",
        "                    loss = lam * loss_a + (1 - lam) * loss_b\n",
        "                else:\n",
        "                    # Standard CE (for Teacher training)\n",
        "                    loss = lam * nn.CrossEntropyLoss()(student_outputs, labels_a) + \\\n",
        "                           (1 - lam) * nn.CrossEntropyLoss()(student_outputs, labels_b)\n",
        "            \n",
        "            # Backward\n",
        "            scaler.scale(loss).backward()\n",
        "            \n",
        "            if grad_clip > 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "                \n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Step Scheduler\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Validation\n",
        "        train_loss = running_loss / len(dataloader)\n",
        "        val_acc, val_loss = evaluate_model_with_loss(model, testloader, val_criterion)\n",
        "        \n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_accuracy'].append(val_acc)\n",
        "        \n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | LR: {current_lr:.6f}\")\n",
        "        \n",
        "        # Save Best\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(model.state_dict(), f\"{MODEL_DIR}/{model_name}.pth\")\n",
        "            epochs_no_improve = 0\n",
        "            print(f\"  New best model saved! Accuracy: {best_acc:.2f}%\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            \n",
        "        # Checkpointing\n",
        "        if (epoch + 1) % CHECKPOINT_FREQUENCY == 0:\n",
        "            save_checkpoint(model, optimizer, scheduler, epoch, best_acc, history, model_name, epochs_no_improve)\n",
        "            cleanup_old_checkpoints(model_name)\n",
        "            \n",
        "        # Early Stopping\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
        "            break\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "    print(f\"\\nTraining complete. Best accuracy: {best_acc:.2f}%\")\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history\n",
        "\n",
        "print(\"Training function loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Initialize Models\n",
        "print(\"Loading Teacher (EfficientNetV2-L)...\")\n",
        "teacher_model = efficientnet_v2_l(weights=EfficientNet_V2_L_Weights.IMAGENET1K_V1)\n",
        "teacher_model.classifier[1] = nn.Linear(teacher_model.classifier[1].in_features, NUM_CLASSES)\n",
        "teacher_model = teacher_model.to(device)\n",
        "\n",
        "print(\"Loading Student (EfficientNetV2-S)...\")\n",
        "student_model = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "student_model.classifier[1] = nn.Linear(student_model.classifier[1].in_features, NUM_CLASSES)\n",
        "student_model = student_model.to(device)\n",
        "\n",
        "print(\"Models loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Train/Load Teacher Model\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEACHER MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "teacher_path = f\"{MODEL_DIR}/teacher_model.pth\"\n",
        "if os.path.exists(teacher_path):\n",
        "    print(f\"Found existing Teacher Model: {teacher_path}\")\n",
        "    teacher_model.load_state_dict(torch.load(teacher_path))\n",
        "else:\n",
        "    print(\"Training Teacher Model (This may take a while)...\")\n",
        "    opt_t = optim.AdamW(teacher_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    sch_t = optim.lr_scheduler.CosineAnnealingLR(opt_t, T_max=NUM_EPOCHS)\n",
        "    teacher_model, teacher_history = train_model_optimized(\n",
        "        teacher_model, trainloader, opt_t, sch_t, NUM_EPOCHS, \"teacher_model\", teacher_model=None\n",
        "    )\n",
        "\n",
        "# Evaluate Teacher\n",
        "teacher_model.eval()\n",
        "teacher_accuracy, _ = evaluate_model_with_loss(teacher_model, testloader, nn.CrossEntropyLoss())\n",
        "print(f\"\\nTeacher Accuracy: {teacher_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Train Distilled Student (SOTA - DKD + CutMix)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DISTILLED STUDENT MODEL (DKD + CutMix/Mixup)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "student_name = \"distilled_student_dkd_sota\"\n",
        "student_path = f\"{MODEL_DIR}/{student_name}.pth\"\n",
        "\n",
        "if os.path.exists(student_path):\n",
        "    print(f\"Found existing Distilled Model: {student_path}\")\n",
        "    student_model.load_state_dict(torch.load(student_path))\n",
        "    distilled_accuracy, _ = evaluate_model_with_loss(student_model, testloader, nn.CrossEntropyLoss())\n",
        "    print(f\"Distilled Student Accuracy: {distilled_accuracy:.2f}%\")\n",
        "else:\n",
        "    print(f\"\\nStarting SOTA Distillation (DKD + CutMix)...\")\n",
        "    print(f\"  DKD Alpha (TCKD): {DKD_ALPHA}\")\n",
        "    print(f\"  DKD Beta (NCKD): {DKD_BETA}\")\n",
        "    print(f\"  Temperature: {TEMPERATURE}\")\n",
        "    \n",
        "    # Optimizer & Scheduler\n",
        "    opt_s = optim.AdamW(student_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    sch_s = optim.lr_scheduler.CosineAnnealingLR(opt_s, T_max=NUM_EPOCHS)\n",
        "    \n",
        "    # Run SOTA Training\n",
        "    trained_student, distilled_history = train_model_optimized(\n",
        "        model=student_model,\n",
        "        dataloader=trainloader,\n",
        "        optimizer=opt_s,\n",
        "        scheduler=sch_s,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        model_name=student_name,\n",
        "        teacher_model=teacher_model,\n",
        "        dkd_alpha=DKD_ALPHA,\n",
        "        dkd_beta=DKD_BETA,\n",
        "        temp=TEMPERATURE,\n",
        "        patience=PATIENCE\n",
        "    )\n",
        "    \n",
        "    distilled_accuracy, _ = evaluate_model_with_loss(trained_student, testloader, nn.CrossEntropyLoss())\n",
        "    print(f\"\\nDistilled Student Final Accuracy: {distilled_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Results Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n| Model                    | Accuracy (%) |\")\n",
        "print(\"|--------------------------|--------------|\")\n",
        "try:\n",
        "    print(f\"| Teacher (EfficientNet-L) | {teacher_accuracy:12.2f} |\")\n",
        "except NameError:\n",
        "    print(f\"| Teacher (EfficientNet-L) | {'N/A':>12} |\")\n",
        "\n",
        "try:\n",
        "    print(f\"| Distilled (DKD+CutMix)   | {distilled_accuracy:12.2f} |\")\n",
        "except NameError:\n",
        "    print(f\"| Distilled (DKD+CutMix)   | {'N/A':>12} |\")\n",
        "\n",
        "try:\n",
        "    improvement = distilled_accuracy - teacher_accuracy\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    if improvement > 0:\n",
        "        print(f\"Student SURPASSED Teacher by: {improvement:+.2f}%\")\n",
        "    else:\n",
        "        print(f\"Gap from Teacher: {improvement:.2f}%\")\n",
        "    print(f\"{'='*80}\")\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "print(f\"\\nAll models saved to: {MODEL_DIR}\")\n",
        "print(f\"Checkpoints saved to: {CHECKPOINT_DIR}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
