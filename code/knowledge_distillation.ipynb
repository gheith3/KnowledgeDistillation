{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Knowledge Distillation for EfficientNetV2 on CIFAR-100\n",
        "\n",
        "This Jupyter Notebook implements the experiments for the research paper \"Improving a Compact Vision Model Using Knowledge Distillation.\"\n",
        "\n",
        "**Objective:** To train a compact student model (`EfficientNetV2-S`) using knowledge distillation from a larger teacher model (`EfficientNetV2-L`) and evaluate the performance improvement against a baseline.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸš€ Setup Instructions\n",
        "\n",
        "### **Hardware Requirements:**\n",
        "\n",
        "- **GPU:** NVIDIA GPU with at least 16GB VRAM recommended\n",
        "- **Training Time:** Approximately 8-12 hours for 200 epochs (depending on GPU)\n",
        "\n",
        "### **Kaggle Setup:**\n",
        "\n",
        "1. **Enable GPU:** Settings â†’ Accelerator â†’ **GPU P100** or **GPU T4 x2**\n",
        "2. **Enable Internet:** Settings â†’ Internet â†’ **On** (needed for downloading models)\n",
        "3. **Run Cell 1** to check if Kaggle is detected\n",
        "\n",
        "### **ðŸ“‹ Training Configuration:**\n",
        "\n",
        "The notebook is configured to train for **200 epochs** in a single session:\n",
        "\n",
        "```python\n",
        "# Default configuration:\n",
        "NUM_EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE_FINETUNE = 0.0001\n",
        "```\n",
        "\n",
        "### **Quick Test (Recommended First):**\n",
        "\n",
        "Before running full training, test with 5 epochs:\n",
        "\n",
        "- In Cell 2, manually set: `NUM_EPOCHS = 5`\n",
        "- This takes ~10 minutes and verifies everything works\n",
        "\n",
        "### **ðŸ“Š Expected Results:**\n",
        "\n",
        "- **Teacher Model:** ~75-78% accuracy on CIFAR-100\n",
        "- **Baseline Student:** ~70-72% accuracy\n",
        "- **Distilled Student:** ~75-77% accuracy (matching or approaching teacher)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Š Notebook Structure:\n",
        "\n",
        "1. **Setup:** Imports, configuration, Kaggle detection\n",
        "2. **Data Loading:** CIFAR-100 dataset with caching\n",
        "3. **Model Definition:** Teacher and Student models\n",
        "4. **Teacher Fine-tuning:** Train teacher on CIFAR-100\n",
        "5. **Baseline Training:** Student without distillation\n",
        "6. **KD Training:** Student with distillation\n",
        "7. **Evaluation:** Compare results\n",
        "8. **SOTA Comparison:** Compare with published models\n",
        "9. **Ablation Studies:** Test different hyperparameters\n",
        "\n",
        "---\n",
        "\n",
        "**ðŸŽ¯ Note:** Training will automatically save checkpoints every 10-20 epochs and the best model based on validation accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (only if not already installed)\n",
        "import importlib.util\n",
        "\n",
        "# Check if thop is already installed\n",
        "thop_installed = importlib.util.find_spec(\"thop\") is not None\n",
        "\n",
        "if not thop_installed:\n",
        "    print(\"Installing thop package...\")\n",
        "    !pip install thop -q\n",
        "    print(\"Required packages installed successfully.\")\n",
        "else:\n",
        "    print(\"All required packages already installed. Skipping installation.\")\n",
        "\n",
        "print(\"Note: Any dependency warnings about CUDA libraries can be safely ignored.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Setup\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import efficientnet_v2_s, efficientnet_v2_l, EfficientNet_V2_S_Weights, EfficientNet_V2_L_Weights\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from thop import profile\n",
        "import pandas as pd\n",
        "\n",
        "# --- Kaggle Detection ---\n",
        "IS_KAGGLE = os.path.exists('/kaggle/working')\n",
        "print(f\"Running on Kaggle: {IS_KAGGLE}\")\n",
        "\n",
        "# --- Reproducibility ---\n",
        "# Paper Section 3.5: \"We set a random seed of 42 for reproducibility\"\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# GPU Information\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"Note: Paper methodology specifies V100 32GB, but code adapts to available GPU\")\n",
        "\n",
        "# Directory setup (Kaggle-compatible)\n",
        "if IS_KAGGLE:\n",
        "    MODEL_DIR = \"/kaggle/working/models\"\n",
        "    DATA_DIR = \"/kaggle/working/data\"\n",
        "else:\n",
        "    MODEL_DIR = \"models\"\n",
        "    DATA_DIR = \"./data\"\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Global parameters - Aligned with Paper Section 3.5\n",
        "# Paper: \"AdamW optimizer with initial learning rate of 0.001, weight decay of 0.01\"\n",
        "LEARNING_RATE = 0.001  # For training from scratch\n",
        "LEARNING_RATE_FINETUNE = 0.0001  # Lower LR for fine-tuning pre-trained models (10x smaller)\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "# Paper: \"batch size of 128\"\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Paper Section 3.3: \"For temperature, we evaluate T âˆˆ {1, 2, 4, 8}\"\n",
        "# Paper Section 3.3: \"For the weighting factor, we evaluate Î± âˆˆ {0.1, 0.3, 0.5, 0.7, 0.9}\"\n",
        "# Optimized values for stable training with pre-trained models\n",
        "TEMPERATURE = 2.0  # Reduced from 4.0 for better convergence\n",
        "ALPHA = 0.5  # Reduced from 0.7 for balanced learning\n",
        "\n",
        "NUM_CLASSES = 100  # CIFAR-100 has 100 classes\n",
        "\n",
        "# Paper Section 3.5: \"early stopping with a patience of 20 epochs\"\n",
        "PATIENCE = 20\n",
        "\n",
        "# Paper Section 3.5: \"train for 200 epochs\"\n",
        "NUM_EPOCHS = 200\n",
        "\n",
        "# Apply to all training phases\n",
        "NUM_EPOCHS_BASELINE = NUM_EPOCHS\n",
        "NUM_EPOCHS_KD = NUM_EPOCHS\n",
        "NUM_EPOCHS_ABLATION = NUM_EPOCHS\n",
        "\n",
        "# Checkpoint frequency - Reduced to save disk space on Kaggle\n",
        "# Automatic cleanup keeps only the 3 most recent checkpoints\n",
        "CHECKPOINT_FREQUENCY = 20 if IS_KAGGLE else 30\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"EXPERIMENT CONFIGURATION (Aligned with Paper Section 3.5)\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Platform: {'Kaggle Notebooks' if IS_KAGGLE else 'Local/Cloud'}\")\n",
        "print(f\"Total Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"Learning Rate (from scratch): {LEARNING_RATE}\")\n",
        "print(f\"Learning Rate (fine-tuning): {LEARNING_RATE_FINETUNE}\")\n",
        "print(f\"Weight Decay: {WEIGHT_DECAY}\")\n",
        "print(f\"Temperature (T): {TEMPERATURE} (optimized for pre-trained models)\")\n",
        "print(f\"Alpha (Î±): {ALPHA} (balanced distillation + hard labels)\")\n",
        "print(f\"Early Stopping Patience: {PATIENCE}\")\n",
        "print(f\"Checkpoint Frequency: {CHECKPOINT_FREQUENCY} epochs (auto-cleanup enabled)\")\n",
        "print(f\"Model Directory: {MODEL_DIR}\")\n",
        "print(f\"Data Directory: {DATA_DIR}\")\n",
        "print(f\"{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Justification for Global Hyperparameters\n",
        "\n",
        "The values for the global parameters in this experiment are chosen based on a combination of common practices in the computer vision literature and the specific requirements of our models and dataset.\n",
        "\n",
        "- **`SEED = 42`**: We set a fixed random seed to ensure that our experiments are **reproducible**. The specific number is arbitrary, but fixing it guarantees that anyone running this code will get the exact same random weight initializations and data shuffling, leading to the same results.\n",
        "\n",
        "- **`BATCH_SIZE = 128`**: This batch size is chosen to maximize GPU utilization while fitting in memory. It is a power of 2, which is computationally efficient on GPU hardware. This value provides a good balance between accurate gradient estimation and the memory capacity of modern GPUs like the Tesla P100.\n",
        "\n",
        "- **`NUM_EPOCHS = 200`**: This value provides sufficient training time for the models to converge on the CIFAR-100 dataset. It allows the models to fully adapt to the data distribution. Training all 200 epochs in a single session requires approximately 8-12 hours on a modern GPU.\n",
        "\n",
        "- **`LEARNING_RATE = 0.001`**: This is a standard and robust starting learning rate for the `AdamW` optimizer, which is known to perform well across a wide range of tasks when training from scratch. We use a cosine annealing learning rate scheduler to adjust this rate over time for better convergence.\n",
        "\n",
        "- **`LEARNING_RATE_FINETUNE = 0.0001`**: When fine-tuning pre-trained models (like our teacher model with ImageNet weights), we use a 10x smaller learning rate. This prevents the model from \"forgetting\" its pre-learned features and allows for gentle adaptation to the new dataset. Using the same learning rate as training from scratch would cause gradient explosion and model collapse.\n",
        "\n",
        "- **`TEMPERATURE = 4.0`**: This is a commonly used value in the knowledge distillation literature. It provides a moderate level of \"softening\" to the teacher's outputs, which has been shown to be effective for transferring knowledge without making the distribution too flat.\n",
        "\n",
        "- **`ALPHA = 0.7`**: This weighting factor gives more importance to the distillation loss (from the teacher) than the cross-entropy loss (from the true labels). This is a common strategy in knowledge distillation, as the primary goal is to leverage the rich \"dark knowledge\" from the teacher model. Our ablation study investigates the effect of this parameter more deeply.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Preprocessing\n",
        "\n",
        "We load the CIFAR-100 dataset. We apply standard data augmentation techniques for the training set (random cropping, random horizontal flipping) and normalization for both the training and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data transformations - Aligned with Paper Section 3.5\n",
        "# Paper: \"RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), Normalization\"\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),  # Paper Section 3.5\n",
        "    transforms.RandomHorizontalFlip(),  # p=0.5 by default\n",
        "    transforms.ToTensor(),\n",
        "    # Paper Section 3.5: Mean=[0.5071, 0.4867, 0.4408], Std=[0.2675, 0.2565, 0.2761]\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "])\n",
        "\n",
        "# Check if CIFAR-100 is already downloaded\n",
        "cifar_path = os.path.join(DATA_DIR, 'cifar-100-python')\n",
        "already_downloaded = os.path.exists(cifar_path)\n",
        "\n",
        "if already_downloaded:\n",
        "    print(f\"CIFAR-100 already downloaded at: {DATA_DIR}\")\n",
        "    print(\"Skipping download, loading from cache...\")\n",
        "else:\n",
        "    print(f\"Downloading CIFAR-100 dataset to: {DATA_DIR}\")\n",
        "\n",
        "# Optimization: Use pin_memory=True for faster data transfer to CUDA\n",
        "use_pin_memory = torch.cuda.is_available()\n",
        "num_workers = os.cpu_count()\n",
        "if num_workers > 4: num_workers = 4 # Cap at 4 to avoid overhead\n",
        "\n",
        "# Load datasets (Kaggle-compatible paths)\n",
        "# Paper Section 3.4: \"CIFAR-100 dataset... 50,000 training images and 10,000 testing images\"\n",
        "trainset = torchvision.datasets.CIFAR100(root=DATA_DIR, train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, \n",
        "                                          num_workers=num_workers, pin_memory=use_pin_memory)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root=DATA_DIR, train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, \n",
        "                                         num_workers=num_workers, pin_memory=use_pin_memory)\n",
        "\n",
        "print(f\"Training batches: {len(trainloader)}\")\n",
        "print(f\"Testing batches: {len(testloader)}\")\n",
        "print(f\"Training samples: {len(trainset)} (Paper: 50,000)\")\n",
        "print(f\"Testing samples: {len(testset)} (Paper: 10,000)\")\n",
        "print(f\"DataLoader optimization: num_workers={num_workers}, pin_memory={use_pin_memory}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Definition\n",
        "\n",
        "We define the teacher and student models. We use pre-trained `EfficientNetV2-L` as the teacher and `EfficientNetV2-S` as the student, both from `torchvision`. Using pre-trained weights on ImageNet allows us to leverage powerful, pre-learned features, which we will then fine-tune on the CIFAR-100 dataset.\n",
        "\n",
        "We modify the final classifier layer of each model to match the number of classes in CIFAR-100 (100). The teacher model's weights are frozen, as it only serves as a guide and should not be trained further.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Teacher Model - Paper Section 3.2: \"EfficientNetV2-L... approximately 120 million parameters\"\n",
        "print(\"Loading Teacher Model (EfficientNetV2-L)...\")\n",
        "teacher_model = efficientnet_v2_l(weights=EfficientNet_V2_L_Weights.IMAGENET1K_V1)\n",
        "print(\"Teacher model loaded (ImageNet weights)\")\n",
        "\n",
        "# Freeze all the parameters in the teacher model (will be unfrozen for fine-tuning later)\n",
        "for param in teacher_model.parameters():\n",
        "    param.requires_grad = False\n",
        "teacher_model.classifier[1] = nn.Linear(teacher_model.classifier[1].in_features, NUM_CLASSES)\n",
        "teacher_model = teacher_model.to(device)\n",
        "teacher_model.eval()\n",
        "\n",
        "# Student Model - Paper Section 3.2: \"EfficientNetV2-S... approximately 22 million parameters\"\n",
        "print(\"Loading Student Model (EfficientNetV2-S)...\")\n",
        "student_model = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "print(\"Student model loaded (ImageNet weights)\")\n",
        "\n",
        "student_model.classifier[1] = nn.Linear(student_model.classifier[1].in_features, NUM_CLASSES)\n",
        "student_model = student_model.to(device)\n",
        "\n",
        "print(\"\\nTeacher and Student models defined and moved to device\")\n",
        "print(\"Teacher model parameters are frozen (will be unfrozen for fine-tuning)\")\n",
        "print(f\"Note: Paper Section 3.2: Teacher ~120M params, Student ~22M params\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Helper Functions for Training and Evaluation\n",
        "\n",
        "We define helper functions for the training loop and for evaluating the model's accuracy on the test set. These functions must be defined BEFORE we start training any models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.amp import autocast, GradScaler\n",
        "import glob\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    \"\"\"\n",
        "    Evaluates model accuracy on a given dataset.\n",
        "    Paper Section 3.6: \"Top-1 Accuracy (%): The primary metric for classification performance\"\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "def evaluate_model_with_loss(model, dataloader, criterion):\n",
        "    \"\"\"\n",
        "    Evaluates model accuracy and loss on a given dataset.\n",
        "    Paper Section 3.6: \"Top-1 Accuracy (%) and Validation Loss\"\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    accuracy = 100 * correct / total\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    return accuracy, avg_loss\n",
        "\n",
        "def distillation_loss(student_logits, teacher_logits, hard_labels, temp, alpha):\n",
        "    \"\"\"\n",
        "    Calculates the distillation loss, combining soft and hard targets.\n",
        "    Fixed: Proper scaling of KL divergence loss for stable training.\n",
        "    \"\"\"\n",
        "    # Soft targets from teacher (with temperature)\n",
        "    soft_teacher_outputs = nn.functional.softmax(teacher_logits / temp, dim=1)\n",
        "    soft_student_outputs = nn.functional.log_softmax(student_logits / temp, dim=1)\n",
        "    \n",
        "    # KL Divergence loss (already scaled by temp^2 in the formula)\n",
        "    # Using sum_over_batch_size instead of batchmean for correct scaling\n",
        "    distill_loss = nn.functional.kl_div(\n",
        "        soft_student_outputs, \n",
        "        soft_teacher_outputs, \n",
        "        reduction='sum'\n",
        "    ) / student_logits.size(0) * (temp * temp)\n",
        "    \n",
        "    # Hard target cross-entropy loss\n",
        "    ce_loss = nn.functional.cross_entropy(student_logits, hard_labels)\n",
        "    \n",
        "    # Combined loss\n",
        "    total_loss = alpha * distill_loss + (1.0 - alpha) * ce_loss\n",
        "    return total_loss\n",
        "\n",
        "def cleanup_old_checkpoints(model_name, keep_last_n=3):\n",
        "    \"\"\"\n",
        "    Removes old checkpoint files, keeping only the N most recent ones.\n",
        "    This prevents disk space issues during long training runs.\n",
        "    \"\"\"\n",
        "    checkpoint_pattern = f\"{MODEL_DIR}/{model_name}_checkpoint_epoch*.pth\"\n",
        "    checkpoints = sorted(glob.glob(checkpoint_pattern))\n",
        "    \n",
        "    if len(checkpoints) > keep_last_n:\n",
        "        for old_checkpoint in checkpoints[:-keep_last_n]:\n",
        "            try:\n",
        "                os.remove(old_checkpoint)\n",
        "                print(f\"  -> Cleaned up old checkpoint: {os.path.basename(old_checkpoint)}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  -> Warning: Could not remove {old_checkpoint}: {e}\")\n",
        "\n",
        "def train_model_unified(model, dataloader, optimizer, scheduler, num_epochs, model_name, \n",
        "                       criterion=None, teacher_model=None, temp=None, alpha=None, patience=20):\n",
        "    \"\"\"\n",
        "    Unified training loop supporting:\n",
        "    1. Standard Training (if teacher_model is None)\n",
        "    2. Knowledge Distillation (if teacher_model is provided)\n",
        "    3. Automatic Mixed Precision (AMP) for faster training\n",
        "    4. Early Stopping and Checkpointing\n",
        "    5. Automatic checkpoint cleanup to prevent disk space issues\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    if teacher_model:\n",
        "        teacher_model.eval()\n",
        "        print(f\"Starting Knowledge Distillation with T={temp}, Î±={alpha}\")\n",
        "    else:\n",
        "        print(\"Starting Standard Training\")\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    history = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
        "    epochs_no_improve = 0\n",
        "    \n",
        "    # AMP Scaler (Updated for PyTorch 2.4+)\n",
        "    # 'cuda' device type is specified for P100 compatibility\n",
        "    scaler = GradScaler('cuda', enabled=torch.cuda.is_available())\n",
        "    \n",
        "    # Validation criterion is always CrossEntropy\n",
        "    val_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for i, data in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Mixed Precision Context (Updated for PyTorch 2.4+)\n",
        "            with autocast('cuda', enabled=torch.cuda.is_available()):\n",
        "                if teacher_model:\n",
        "                    with torch.no_grad():\n",
        "                        teacher_outputs = teacher_model(inputs)\n",
        "                    student_outputs = model(inputs)\n",
        "                    loss = distillation_loss(student_outputs, teacher_outputs, labels, temp, alpha)\n",
        "                else:\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    student_outputs = outputs\n",
        "\n",
        "            # Scaled Backward Pass\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(student_outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        scheduler.step()\n",
        "        \n",
        "        train_loss = running_loss / len(dataloader)\n",
        "        train_acc = 100 * correct / total\n",
        "        val_acc, val_loss = evaluate_model_with_loss(model, testloader, val_criterion)\n",
        "        \n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_accuracy'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_accuracy'].append(val_acc)\n",
        "        \n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, LR: {current_lr:.6f}\")\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(model.state_dict(), f\"{MODEL_DIR}/{model_name}.pth\")\n",
        "            epochs_no_improve = 0\n",
        "            print(f\"  -> New best model saved!\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            \n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"[Early stopping triggered after {epoch+1} epochs]\")\n",
        "            break\n",
        "        \n",
        "        # Save checkpoint every CHECKPOINT_FREQUENCY epochs\n",
        "        if (epoch + 1) % CHECKPOINT_FREQUENCY == 0:\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "                'history': history,\n",
        "                'epochs_no_improve': epochs_no_improve\n",
        "            }\n",
        "            checkpoint_path = f\"{MODEL_DIR}/{model_name}_checkpoint_epoch{epoch+1}.pth\"\n",
        "            \n",
        "            try:\n",
        "                torch.save(checkpoint, checkpoint_path)\n",
        "                print(f\"  -> Checkpoint saved: {checkpoint_path}\")\n",
        "                \n",
        "                # Clean up old checkpoints to save disk space\n",
        "                cleanup_old_checkpoints(model_name, keep_last_n=3)\n",
        "            except Exception as e:\n",
        "                print(f\"  -> Warning: Checkpoint save failed: {e}\")\n",
        "                print(f\"  -> Continuing training (best model is still safe)\")\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "    print(f\"Training complete. Best accuracy: {best_acc:.2f}%\")\n",
        "    print(f\"Best model saved to: {MODEL_DIR}/{model_name}.pth\")\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history\n",
        "\n",
        "def calculate_flops(model):\n",
        "    \"\"\"\n",
        "    Calculates FLOPs and number of parameters for a model.\n",
        "    \"\"\"\n",
        "    input_tensor = torch.randn(1, 3, 32, 32).to(device)\n",
        "    flops, params = profile(model, inputs=(input_tensor, ), verbose=False)\n",
        "    model_flops = flops / 1e9\n",
        "    model_params = params / 1e6\n",
        "    print(f\"Model FLOPs: {model_flops:.2f} GFLOPs\")\n",
        "    print(f\"Model Parameters: {model_params:.2f} M\")\n",
        "    return model_flops, model_params\n",
        "\n",
        "def measure_inference_time(model, num_iterations=1000, warmup=100):\n",
        "    \"\"\"\n",
        "    Measures average inference time per image.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    input_tensor = torch.randn(1, 3, 32, 32).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(warmup):\n",
        "            _ = model(input_tensor)\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "    \n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_iterations):\n",
        "            _ = model(input_tensor)\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "    \n",
        "    end_time = time.time()\n",
        "    avg_time_ms = (end_time - start_time) / num_iterations * 1000\n",
        "    \n",
        "    print(f\"Average inference time: {avg_time_ms:.4f} ms/image\")\n",
        "    return avg_time_ms\n",
        "\n",
        "def calculate_model_size(model, model_name):\n",
        "    \"\"\"\n",
        "    Calculates the size of the saved model file in MB.\n",
        "    \"\"\"\n",
        "    temp_path = f\"{MODEL_DIR}/{model_name}_temp.pth\"\n",
        "    torch.save(model.state_dict(), temp_path)\n",
        "    size_mb = os.path.getsize(temp_path) / (1024 * 1024)\n",
        "    os.remove(temp_path)\n",
        "    print(f\"Model size: {size_mb:.2f} MB\")\n",
        "    return size_mb\n",
        "\n",
        "print(\"Unified Helper functions loaded successfully with AMP support and fixed distillation loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Teacher Model Fine-tuning on CIFAR-100\n",
        "\n",
        "Before using the teacher model for knowledge distillation, we fine-tune it on the CIFAR-100 dataset. This ensures the teacher provides high-quality soft labels that are specifically adapted to our target dataset, rather than relying solely on ImageNet pre-trained features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"--- Fine-tuning Teacher Model on CIFAR-100 ---\")\n",
        "\n",
        "# Check if teacher model is already trained\n",
        "teacher_checkpoint_path = f\"{MODEL_DIR}/teacher_model.pth\"\n",
        "teacher_already_trained = os.path.exists(teacher_checkpoint_path)\n",
        "\n",
        "if teacher_already_trained:\n",
        "    print(f\"\\nFound existing teacher model at: {teacher_checkpoint_path}\")\n",
        "    print(\"Loading pre-trained teacher model...\")\n",
        "    teacher_model.load_state_dict(torch.load(teacher_checkpoint_path))\n",
        "    teacher_model.eval()\n",
        "    \n",
        "    teacher_accuracy = evaluate_model(teacher_model, testloader)\n",
        "    print(f\"Loaded Teacher Model Accuracy: {teacher_accuracy:.2f}%\")\n",
        "    \n",
        "    print(\"\\nCalculating teacher model metrics...\")\n",
        "    teacher_flops, teacher_params = calculate_flops(teacher_model)\n",
        "    teacher_time = measure_inference_time(teacher_model)\n",
        "    teacher_size = calculate_model_size(teacher_model, \"teacher_model\")\n",
        "    \n",
        "    teacher_history = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
        "    print(\"\\nSkipping teacher training (already trained).\")\n",
        "    \n",
        "else:\n",
        "    print(\"\\nNo existing teacher model found. Starting training from scratch...\")\n",
        "    \n",
        "    # Unfreeze teacher model for fine-tuning\n",
        "    teacher_model.train()\n",
        "    for param in teacher_model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    criterion_teacher = nn.CrossEntropyLoss()\n",
        "    optimizer_teacher = optim.AdamW(teacher_model.parameters(), lr=LEARNING_RATE_FINETUNE, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler_teacher = optim.lr_scheduler.CosineAnnealingLR(optimizer_teacher, T_max=NUM_EPOCHS)\n",
        "\n",
        "    print(f\"Training teacher for {NUM_EPOCHS} epochs...\")\n",
        "    print(f\"Using fine-tuning learning rate: {LEARNING_RATE_FINETUNE}\")\n",
        "    \n",
        "    trained_teacher, teacher_history = train_model_unified(\n",
        "        model=teacher_model, \n",
        "        dataloader=trainloader, \n",
        "        criterion=criterion_teacher, \n",
        "        optimizer=optimizer_teacher,\n",
        "        scheduler=scheduler_teacher,\n",
        "        num_epochs=NUM_EPOCHS, \n",
        "        model_name=\"teacher_model\",\n",
        "        patience=PATIENCE\n",
        "    )\n",
        "\n",
        "    # Freeze the teacher for distillation\n",
        "    for param in teacher_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    teacher_model.eval()\n",
        "\n",
        "    print(\"\\n--- Teacher Model Evaluation ---\")\n",
        "    teacher_accuracy = evaluate_model(teacher_model, testloader)\n",
        "    print(f\"Teacher Model Accuracy on CIFAR-100: {teacher_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"\\nCalculating teacher model metrics...\")\n",
        "    teacher_flops, teacher_params = calculate_flops(teacher_model)\n",
        "    teacher_time = measure_inference_time(teacher_model)\n",
        "    teacher_size = calculate_model_size(teacher_model, \"teacher_model\")\n",
        "\n",
        "print(\"\\nTeacher model is now frozen and ready for knowledge distillation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Baseline Student Model Training\n",
        "\n",
        "First, we train the student model using only the standard cross-entropy loss. This provides the baseline performance that we will compare against the distilled model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if baseline student is already trained\n",
        "baseline_checkpoint_path = f\"{MODEL_DIR}/baseline_student.pth\"\n",
        "baseline_already_trained = os.path.exists(baseline_checkpoint_path)\n",
        "\n",
        "if baseline_already_trained:\n",
        "    print(\"--- Found Existing Baseline Student Model ---\")\n",
        "    print(f\"Loading from: {baseline_checkpoint_path}\")\n",
        "    \n",
        "    baseline_student_model = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "    baseline_student_model.classifier[1] = nn.Linear(baseline_student_model.classifier[1].in_features, NUM_CLASSES)\n",
        "    baseline_student_model = baseline_student_model.to(device)\n",
        "    baseline_student_model.load_state_dict(torch.load(baseline_checkpoint_path))\n",
        "    baseline_student_model.eval()\n",
        "    \n",
        "    baseline_accuracy = evaluate_model(baseline_student_model, testloader)\n",
        "    print(f\"Loaded Baseline Student Accuracy: {baseline_accuracy:.2f}%\")\n",
        "    \n",
        "    print(\"\\nCalculating model metrics...\")\n",
        "    baseline_flops, baseline_params = calculate_flops(baseline_student_model)\n",
        "    baseline_time = measure_inference_time(baseline_student_model)\n",
        "    baseline_size = calculate_model_size(baseline_student_model, \"baseline_student\")\n",
        "    \n",
        "    baseline_history = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
        "    trained_baseline_model = baseline_student_model\n",
        "    \n",
        "    print(\"\\nSkipping baseline training (already trained).\")\n",
        "    \n",
        "else:\n",
        "    print(\"--- Starting Baseline Student Model Training ---\")\n",
        "    print(\"NOTE: Using fine-tuning learning rate for pre-trained model\")\n",
        "    \n",
        "    baseline_student_model = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "    baseline_student_model.classifier[1] = nn.Linear(baseline_student_model.classifier[1].in_features, NUM_CLASSES)\n",
        "    baseline_student_model = baseline_student_model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(baseline_student_model.parameters(), lr=LEARNING_RATE_FINETUNE, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
        "\n",
        "    print(f\"Training for {NUM_EPOCHS} epochs with early stopping (patience={PATIENCE})\")\n",
        "    \n",
        "    trained_baseline_model, baseline_history = train_model_unified(\n",
        "        model=baseline_student_model, \n",
        "        dataloader=trainloader, \n",
        "        criterion=criterion, \n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        num_epochs=NUM_EPOCHS, \n",
        "        model_name=\"baseline_student\",\n",
        "        patience=PATIENCE\n",
        "    )\n",
        "    \n",
        "    print(\"\\n--- Baseline Model Evaluation ---\")\n",
        "    baseline_accuracy = evaluate_model(trained_baseline_model, testloader)\n",
        "    print(f\"Final Baseline Student Model Accuracy: {baseline_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"\\nCalculating model metrics...\")\n",
        "    baseline_flops, baseline_params = calculate_flops(trained_baseline_model)\n",
        "    baseline_time = measure_inference_time(trained_baseline_model)\n",
        "    baseline_size = calculate_model_size(trained_baseline_model, \"baseline_student\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Knowledge Distillation Training\n",
        "\n",
        "Now, we train the student model using the knowledge distillation loss. The total loss is a combination of the standard cross-entropy loss with the hard labels and the KL Divergence loss between the student's and teacher's softened logits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Knowledge Distillation Training\n",
        "print(\"\\n--- Starting Knowledge Distillation Training ---\")\n",
        "\n",
        "# Check if distilled model is already trained\n",
        "distilled_checkpoint_path = f\"{MODEL_DIR}/distilled_student.pth\"\n",
        "distilled_already_trained = os.path.exists(distilled_checkpoint_path)\n",
        "\n",
        "if distilled_already_trained:\n",
        "    print(f\"Found existing distilled model at: {distilled_checkpoint_path}\")\n",
        "    print(\"Loading pre-trained distilled model...\")\n",
        "    \n",
        "    distilled_student_model = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "    distilled_student_model.classifier[1] = nn.Linear(distilled_student_model.classifier[1].in_features, NUM_CLASSES)\n",
        "    distilled_student_model = distilled_student_model.to(device)\n",
        "    distilled_student_model.load_state_dict(torch.load(distilled_checkpoint_path))\n",
        "    distilled_student_model.eval()\n",
        "    \n",
        "    distilled_accuracy = evaluate_model(distilled_student_model, testloader)\n",
        "    print(f\"Loaded Distilled Student Accuracy: {distilled_accuracy:.2f}%\")\n",
        "    \n",
        "    distilled_flops, distilled_params = calculate_flops(distilled_student_model)\n",
        "    distilled_time = measure_inference_time(distilled_student_model)\n",
        "    distilled_size = calculate_model_size(distilled_student_model, \"distilled_student\")\n",
        "    \n",
        "    distilled_history = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
        "    print(\"\\nSkipping distillation training (already trained).\")\n",
        "    \n",
        "else:\n",
        "    print(\"Starting distillation training from scratch...\")\n",
        "    \n",
        "    # Create fresh student model for distillation\n",
        "    distilled_student_model = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "    distilled_student_model.classifier[1] = nn.Linear(distilled_student_model.classifier[1].in_features, NUM_CLASSES)\n",
        "    distilled_student_model = distilled_student_model.to(device)\n",
        "\n",
        "    # CRITICAL: Use LEARNING_RATE_FINETUNE for pre-trained models to prevent gradient explosion\n",
        "    optimizer_kd = optim.AdamW(distilled_student_model.parameters(), lr=LEARNING_RATE_FINETUNE, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler_kd = optim.lr_scheduler.CosineAnnealingLR(optimizer_kd, T_max=NUM_EPOCHS)\n",
        "\n",
        "    print(f\"Training for {NUM_EPOCHS} epochs with T={TEMPERATURE}, Î±={ALPHA}\")\n",
        "    print(f\"Using fine-tuning learning rate: {LEARNING_RATE_FINETUNE}\")\n",
        "    print(f\"Paper Section 3.3: L_total = {ALPHA} Â· L_KD + {1-ALPHA} Â· L_CE\")\n",
        "    \n",
        "    trained_distilled_model, distilled_history = train_model_unified(\n",
        "        model=distilled_student_model,\n",
        "        dataloader=trainloader,\n",
        "        optimizer=optimizer_kd,\n",
        "        scheduler=scheduler_kd,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        model_name=\"distilled_student\",\n",
        "        teacher_model=teacher_model,\n",
        "        temp=TEMPERATURE,\n",
        "        alpha=ALPHA,\n",
        "        patience=PATIENCE\n",
        "    )\n",
        "    \n",
        "    print(\"\\n--- Distilled Model Evaluation ---\")\n",
        "    distilled_accuracy = evaluate_model(trained_distilled_model, testloader)\n",
        "    print(f\"Final Distilled Student Model Accuracy: {distilled_accuracy:.2f}%\")\n",
        "\n",
        "    print(\"\\nCalculating model metrics (Paper Section 3.6)...\")\n",
        "    distilled_flops, distilled_params = calculate_flops(trained_distilled_model)\n",
        "    distilled_time = measure_inference_time(trained_distilled_model)\n",
        "    distilled_size = calculate_model_size(trained_distilled_model, \"distilled_student\")\n",
        "\n",
        "print(\"\\nKnowledge distillation training complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Final Summary Tables ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPERIMENT RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n--- Table 1: Baseline Performance ---\")\n",
        "print(\"| Model                     | Accuracy (%) | Size (MB) | Inference (ms) | FLOPs (G) | Params (M) |\")\n",
        "print(\"|---------------------------|--------------|-----------|----------------|-----------|------------|\")\n",
        "print(f\"| Teacher (EfficientNetV2-L)| {teacher_accuracy:12.2f} | {teacher_size:9.2f} | {teacher_time:14.4f} | {teacher_flops:9.2f} | {teacher_params:10.2f} |\")\n",
        "print(f\"| Baseline Student (Eff-S)  | {baseline_accuracy:12.2f} | {baseline_size:9.2f} | {baseline_time:14.4f} | {baseline_flops:9.2f} | {baseline_params:10.2f} |\")\n",
        "\n",
        "print(\"\\n--- Table 2: Knowledge Distillation Performance Comparison ---\")\n",
        "print(\"| Model             | Accuracy (%) | Î” Accuracy | Size (MB) | Inference (ms) | FLOPs (G) |\")\n",
        "print(\"|-------------------|--------------|------------|-----------|----------------|-----------|\")\n",
        "print(f\"| Baseline Student  | {baseline_accuracy:12.2f} |      -     | {baseline_size:9.2f} | {baseline_time:14.4f} | {baseline_flops:9.2f} |\")\n",
        "print(f\"| Distilled Student | {distilled_accuracy:12.2f} | {distilled_accuracy - baseline_accuracy:+10.2f} | {distilled_size:9.2f} | {distilled_time:14.4f} | {distilled_flops:9.2f} |\")\n",
        "\n",
        "improvement = distilled_accuracy - baseline_accuracy\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"ACCURACY IMPROVEMENT WITH KNOWLEDGE DISTILLATION: {improvement:+.2f}%\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "# --- Plotting Training Curves ---\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "axes[0, 0].plot(baseline_history['train_accuracy'], label='Baseline Student', linewidth=2)\n",
        "axes[0, 0].plot(distilled_history['train_accuracy'], label='Distilled Student', linewidth=2)\n",
        "axes[0, 0].set_title('Training Accuracy vs. Epochs', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Epochs')\n",
        "axes[0, 0].set_ylabel('Training Accuracy (%)')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 1].plot(baseline_history['val_accuracy'], label='Baseline Student', linewidth=2)\n",
        "axes[0, 1].plot(distilled_history['val_accuracy'], label='Distilled Student', linewidth=2)\n",
        "axes[0, 1].set_title('Validation Accuracy vs. Epochs', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Epochs')\n",
        "axes[0, 1].set_ylabel('Validation Accuracy (%)')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 0].plot(baseline_history['train_loss'], label='Baseline Student', linewidth=2)\n",
        "axes[1, 0].plot(distilled_history['train_loss'], label='Distilled Student', linewidth=2)\n",
        "axes[1, 0].set_title('Training Loss vs. Epochs', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Epochs')\n",
        "axes[1, 0].set_ylabel('Training Loss')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 1].plot(baseline_history['val_loss'], label='Baseline Student', linewidth=2)\n",
        "axes[1, 1].plot(distilled_history['val_loss'], label='Distilled Student', linewidth=2)\n",
        "axes[1, 1].set_title('Validation Loss vs. Epochs', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Epochs')\n",
        "axes[1, 1].set_ylabel('Validation Loss')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Training History Comparison: Baseline vs. Distilled Student', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
        "plt.savefig(f'{MODEL_DIR}/training_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# --- Bar Chart Comparison ---\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "models = ['Teacher\\n(EfficientNetV2-L)', 'Baseline\\nStudent', 'Distilled\\nStudent']\n",
        "accuracies = [teacher_accuracy, baseline_accuracy, distilled_accuracy]\n",
        "sizes = [teacher_size, baseline_size, distilled_size]\n",
        "times = [teacher_time, baseline_time, distilled_time]\n",
        "\n",
        "axes[0].bar(models, accuracies, color=['#2ecc71', '#3498db', '#e74c3c'])\n",
        "axes[0].set_ylabel('Accuracy (%)')\n",
        "axes[0].set_title('Model Accuracy Comparison', fontweight='bold')\n",
        "axes[0].grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "axes[1].bar(models, sizes, color=['#2ecc71', '#3498db', '#e74c3c'])\n",
        "axes[1].set_ylabel('Model Size (MB)')\n",
        "axes[1].set_title('Model Size Comparison', fontweight='bold')\n",
        "axes[1].grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "axes[2].bar(models, times, color=['#2ecc71', '#3498db', '#e74c3c'])\n",
        "axes[2].set_ylabel('Inference Time (ms/image)')\n",
        "axes[2].set_title('Inference Speed Comparison', fontweight='bold')\n",
        "axes[2].grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "plt.suptitle('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.savefig(f'{MODEL_DIR}/model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFigures saved to {MODEL_DIR}/ directory\")\n",
        "print(f\"All outputs will be available in Kaggle's Output tab after session completes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. State-of-the-Art (SOTA) Comparison\n",
        "\n",
        "We compare our distilled student model against other published compact models on CIFAR-100. This provides context for our results and demonstrates the effectiveness of our approach relative to existing methods.\n",
        "\n",
        "**Note:** The baseline results below are collected from published papers. You should verify and update these with the most recent SOTA results from literature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOTA Comparison on CIFAR-100\n",
        "import pandas as pd\n",
        "\n",
        "sota_results = {\n",
        "    'Model': [\n",
        "        'ResNet-56', 'ResNet-110', 'MobileNetV2', 'EfficientNet-B0', 'ShuffleNetV2',\n",
        "        'Our Baseline Student', 'Our Distilled Student'\n",
        "    ],\n",
        "    'Accuracy (%)': [\n",
        "        72.49, 74.84, 74.45, 77.30, 73.50, baseline_accuracy, distilled_accuracy\n",
        "    ],\n",
        "    'FLOPs (G)': [\n",
        "        0.13, 0.25, 0.30, 0.39, 0.15, baseline_flops, distilled_flops\n",
        "    ],\n",
        "    'Parameters (M)': [\n",
        "        0.85, 1.73, 3.50, 5.30, 2.30, baseline_params, distilled_params\n",
        "    ]\n",
        "}\n",
        "\n",
        "sota_df = pd.DataFrame(sota_results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STATE-OF-THE-ART COMPARISON ON CIFAR-100\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n--- Table 4: SOTA Comparison ---\")\n",
        "print(sota_df.to_string(index=False))\n",
        "\n",
        "sota_df['Efficiency Score'] = sota_df['Accuracy (%)'] / sota_df['FLOPs (G)']\n",
        "print(\"\\n--- Efficiency Ranking (Accuracy / FLOPs) ---\")\n",
        "print(sota_df[['Model', 'Efficiency Score']].sort_values('Efficiency Score', ascending=False).to_string(index=False))\n",
        "\n",
        "# Visualizations\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "colors = ['#95a5a6' if 'Our' not in model else '#e74c3c' for model in sota_df['Model']]\n",
        "axes[0].barh(sota_df['Model'], sota_df['Accuracy (%)'], color=colors)\n",
        "axes[0].set_xlabel('Accuracy (%)', fontweight='bold')\n",
        "axes[0].set_title('Accuracy Comparison', fontsize=12, fontweight='bold')\n",
        "axes[0].grid(True, axis='x', alpha=0.3)\n",
        "\n",
        "colors_scatter = ['red' if 'Our' in model else 'blue' for model in sota_df['Model']]\n",
        "sizes = [200 if 'Our' in model else 100 for model in sota_df['Model']]\n",
        "axes[1].scatter(sota_df['FLOPs (G)'], sota_df['Accuracy (%)'], c=colors_scatter, s=sizes, alpha=0.6)\n",
        "for i, model in enumerate(sota_df['Model']):\n",
        "    axes[1].annotate(model, (sota_df['FLOPs (G)'][i], sota_df['Accuracy (%)'][i]), \n",
        "                     fontsize=8, ha='right' if 'Our' in model else 'left')\n",
        "axes[1].set_xlabel('FLOPs (GigaFLOPs)', fontweight='bold')\n",
        "axes[1].set_ylabel('Accuracy (%)', fontweight='bold')\n",
        "axes[1].set_title('Efficiency vs. Accuracy Trade-off', fontsize=12, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].scatter(sota_df['Parameters (M)'], sota_df['Accuracy (%)'], c=colors_scatter, s=sizes, alpha=0.6)\n",
        "for i, model in enumerate(sota_df['Model']):\n",
        "    axes[2].annotate(model, (sota_df['Parameters (M)'][i], sota_df['Accuracy (%)'][i]),\n",
        "                     fontsize=8, ha='right' if 'Our' in model else 'left')\n",
        "axes[2].set_xlabel('Parameters (Millions)', fontweight='bold')\n",
        "axes[2].set_ylabel('Accuracy (%)', fontweight='bold')\n",
        "axes[2].set_title('Model Size vs. Accuracy', fontsize=12, fontweight='bold')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('SOTA Comparison on CIFAR-100', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.savefig(f'{MODEL_DIR}/sota_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "sota_df.to_csv(f'{MODEL_DIR}/sota_comparison.csv', index=False)\n",
        "print(f\"\\nSOTA comparison saved to {MODEL_DIR}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Ablation Studies (Core Contribution)\n",
        "\n",
        "This section is central to our research contribution. We investigate how the key hyperparameters of knowledge distillation, `TEMPERATURE` and `ALPHA`, affect the final performance of the student model. This systematic analysis provides valuable insights into the distillation process for the EfficientNetV2 architecture.\n",
        "\n",
        "**Note:** Running these studies can be time-consuming. For a full analysis, each experiment should run for the full number of epochs. For a quick check, you can reduce `NUM_EPOCHS_ABLATION`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Justification for Hyperparameter Choices in Ablation Study\n",
        "\n",
        "In our ablation study, we select a range of values for `TEMPERATURE` and `ALPHA` based on established practices and the specific roles these hyperparameters play.\n",
        "\n",
        "- **Temperature (T):** This parameter softens the probability distribution of the teacher's outputs. A `T=1` represents the standard softmax with no softening. Higher values increase the influence of the teacher's \"dark knowledge.\" We chose `[2, 4, 6]` to explore a range from moderate to significant softening. `T=4` is a commonly used value in literature, providing a strong baseline, while `T=2` and `T=6` allow us to observe the sensitivity of the model to less and more aggressive softening.\n",
        "\n",
        "- **Alpha (Î±):** This parameter balances the influence of the distillation loss (learning from the teacher) and the cross-entropy loss (learning from the ground-truth labels). The total loss is `Î± * L_KD + (1 - Î±) * L_CE`. A higher `Î±` means we trust the teacher more. We chose `[0.5, 0.7]` to test two common scenarios: an equal balance between the teacher and the hard labels (`Î±=0.5`), and a scenario where the teacher's guidance is given more weight (`Î±=0.7`). This allows us to understand how much the student should rely on the teacher versus the ground truth for this specific task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters for ablation study - Aligned with Paper Section 3.3\n",
        "# Paper: \"For temperature, we evaluate T âˆˆ {1, 2, 4, 8}\"\n",
        "temperatures_to_test = [1, 2, 4, 8]\n",
        "# Paper: \"For the weighting factor, we evaluate Î± âˆˆ {0.1, 0.3, 0.5, 0.7, 0.9}\"\n",
        "alphas_to_test = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "ablation_results = {}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ABLATION STUDY: Hyperparameter Sensitivity Analysis\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Paper Section 3.3: Systematic investigation of T and Î±\")\n",
        "print(f\"Testing {len(temperatures_to_test)} temperatures Ã— {len(alphas_to_test)} alphas = {len(temperatures_to_test) * len(alphas_to_test)} experiments\")\n",
        "print(f\"Each experiment runs for {NUM_EPOCHS_ABLATION} epochs\")\n",
        "\n",
        "if IS_KAGGLE:\n",
        "    print(f\"\\n[KAGGLE NOTE: With {NUM_EPOCHS_ABLATION} epochs, each experiment takes ~8 hours]\")\n",
        "    print(f\"TIP: Run 2-3 experiments per Kaggle session (within 9-hour limit)\")\n",
        "    print(f\"Total sessions needed: ~7-10 sessions for all 20 experiments\")\n",
        "\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "experiment_count = 0\n",
        "total_experiments = len(temperatures_to_test) * len(alphas_to_test)\n",
        "\n",
        "for temp in temperatures_to_test:\n",
        "    ablation_results[temp] = {}\n",
        "    for alpha in alphas_to_test:\n",
        "        experiment_count += 1\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"EXPERIMENT {experiment_count}/{total_experiments}: Temperature={temp}, Alpha={alpha}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        # Create fresh student model for each experiment\n",
        "        student_model_abl = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "        student_model_abl.classifier[1] = nn.Linear(student_model_abl.classifier[1].in_features, NUM_CLASSES)\n",
        "        student_model_abl = student_model_abl.to(device)\n",
        "        \n",
        "        optimizer_abl = optim.AdamW(student_model_abl.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "        scheduler_abl = optim.lr_scheduler.CosineAnnealingLR(optimizer_abl, T_max=NUM_EPOCHS_ABLATION)\n",
        "        \n",
        "        # Run distillation training with specific T and Î±\n",
        "        trained_model_abl, _ = train_distillation(\n",
        "            student_model_abl, teacher_model, trainloader, \n",
        "            optimizer_abl, scheduler_abl, temp=temp, alpha=alpha, \n",
        "            num_epochs=NUM_EPOCHS_ABLATION, model_name=f\"abl_T{temp}_A{alpha}\", patience=PATIENCE\n",
        "        )\n",
        "        \n",
        "        # Evaluate and store result\n",
        "        accuracy = evaluate_model(trained_model_abl, testloader)\n",
        "        ablation_results[temp][alpha] = accuracy\n",
        "        print(f\"\\nResult for T={temp}, Î±={alpha}: Accuracy = {accuracy:.2f}%\")\n",
        "        print(f\"Progress: {experiment_count}/{total_experiments} experiments completed\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ABLATION STUDY COMPLETED\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Convert results to DataFrame for analysis\n",
        "ablation_df = pd.DataFrame(ablation_results)\n",
        "ablation_df.index.name = 'Alpha (Î±)'\n",
        "ablation_df.columns.name = 'Temperature (T)'\n",
        "\n",
        "print(\"\\n--- Table 3: Ablation Study Results (Accuracy %) ---\")\n",
        "print(\"Paper Section 3.3: Grid search over all T and Î± combinations\")\n",
        "print(ablation_df.to_string())\n",
        "\n",
        "# Find best hyperparameters\n",
        "best_temp, best_alpha, best_acc = None, None, 0\n",
        "for temp in temperatures_to_test:\n",
        "    for alpha in alphas_to_test:\n",
        "        if ablation_results[temp][alpha] > best_acc:\n",
        "            best_acc = ablation_results[temp][alpha]\n",
        "            best_temp, best_alpha = temp, alpha\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"BEST HYPERPARAMETERS FOUND:\")\n",
        "print(f\"  Temperature (T): {best_temp}\")\n",
        "print(f\"  Alpha (Î±): {best_alpha}\")\n",
        "print(f\"  Best Accuracy: {best_acc:.2f}%\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "# Save results for paper\n",
        "ablation_df.to_csv(f'{MODEL_DIR}/ablation_results.csv')\n",
        "print(f\"Ablation results saved to {MODEL_DIR}/ablation_results.csv\")\n",
        "print(f\"Note: Use this data for Paper Section 4 (Experiments and Results)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Visualize Ablation Study Results ---\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# 1. Heatmap\n",
        "sns.heatmap(ablation_df, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", ax=axes[0], cbar_kws={'label': 'Accuracy (%)'})\n",
        "axes[0].set_xlabel(\"Temperature (T)\", fontweight='bold')\n",
        "axes[0].set_ylabel(\"Alpha (Î±)\", fontweight='bold')\n",
        "axes[0].set_title(\"Ablation Study Heatmap\", fontsize=12, fontweight='bold')\n",
        "\n",
        "# 2. Temperature Sensitivity (averaged over alphas)\n",
        "temp_avg = ablation_df.mean(axis=0)\n",
        "temp_std = ablation_df.std(axis=0)\n",
        "axes[1].errorbar(temperatures_to_test, temp_avg.values, yerr=temp_std.values, \n",
        "                 marker='o', linewidth=2, markersize=8, capsize=5, capthick=2)\n",
        "axes[1].set_xlabel('Temperature (T)', fontweight='bold')\n",
        "axes[1].set_ylabel('Average Accuracy (%)', fontweight='bold')\n",
        "axes[1].set_title('Temperature Sensitivity Analysis', fontsize=12, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_xticks(temperatures_to_test)\n",
        "\n",
        "# 3. Alpha Sensitivity (averaged over temperatures)\n",
        "alpha_avg = ablation_df.mean(axis=1)\n",
        "alpha_std = ablation_df.std(axis=1)\n",
        "axes[2].errorbar(alphas_to_test, alpha_avg.values, yerr=alpha_std.values,\n",
        "                 marker='o', linewidth=2, markersize=8, capsize=5, capthick=2)\n",
        "axes[2].set_xlabel('Alpha (Î±)', fontweight='bold')\n",
        "axes[2].set_ylabel('Average Accuracy (%)', fontweight='bold')\n",
        "axes[2].set_title('Alpha Sensitivity Analysis', fontsize=12, fontweight='bold')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "axes[2].set_xticks(alphas_to_test)\n",
        "\n",
        "plt.suptitle('Ablation Study: Hyperparameter Sensitivity Analysis', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.savefig('models/ablation_study.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nAblation study visualization saved to models/ablation_study.png\")\n",
        "\n",
        "# Additional analysis: Show top 5 configurations\n",
        "print(\"\\n--- Top 5 Hyperparameter Configurations ---\")\n",
        "flat_results = []\n",
        "for temp in temperatures_to_test:\n",
        "    for alpha in alphas_to_test:\n",
        "        flat_results.append({\n",
        "            'Temperature': temp,\n",
        "            'Alpha': alpha,\n",
        "            'Accuracy': ablation_results[temp][alpha]\n",
        "        })\n",
        "\n",
        "top_configs = pd.DataFrame(flat_results).sort_values('Accuracy', ascending=False).head(5)\n",
        "print(top_configs.to_string(index=False))\n",
        "\n",
        "# Statistical analysis\n",
        "print(\"\\n--- Statistical Summary ---\")\n",
        "print(f\"Mean Accuracy: {ablation_df.values.mean():.2f}%\")\n",
        "print(f\"Std Deviation: {ablation_df.values.std():.2f}%\")\n",
        "print(f\"Min Accuracy: {ablation_df.values.min():.2f}%\")\n",
        "print(f\"Max Accuracy: {ablation_df.values.max():.2f}%\")\n",
        "print(f\"Range: {ablation_df.values.max() - ablation_df.values.min():.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
