{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Knowledge Distillation Experiments - Unified Framework\n",
        "\n",
        "## Master's Thesis: Robust Knowledge Distillation for Compact Vision Models\n",
        "\n",
        "**Author:** Gheith Alrawahi  \n",
        "**Institution:** Nankai University  \n",
        "**Supervisor:** Prof. Jing Wang\n",
        "\n",
        "---\n",
        "\n",
        "### Experiments Overview:\n",
        "\n",
        "| ID   | Name       | Method       | Key Feature                     |\n",
        "| :--- | :--------- | :----------- | :------------------------------ |\n",
        "| v1   | Baseline   | Standard KD  | Mixup + CutMix only             |\n",
        "| v2   | Enhanced   | Standard KD  | + AutoAugment + Label Smoothing |\n",
        "| v3   | DKD β=8.0  | Decoupled KD | Default DKD parameters          |\n",
        "| v3.1 | DKD β=2.0  | Decoupled KD | Tuned beta parameter            |\n",
        "| v4   | Saturation | Standard KD  | Strong teacher + Standard KD    |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "GPU: NVIDIA GeForce RTX 5070 Laptop GPU\n",
            "Memory: 8.5 GB\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Imports and Setup\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import copy\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Local imports\n",
        "from config import (\n",
        "    RESULTS_DIR, MODELS_DIR, CHECKPOINTS_DIR,\n",
        "    get_experiment, ALL_EXPERIMENTS,\n",
        "    ExperimentConfig\n",
        ")\n",
        "from utils import (\n",
        "    set_seed, mixup_data, cutmix_data,\n",
        "    kd_loss_with_mixup, evaluate_model,\n",
        "    save_checkpoint, load_checkpoint, cleanup_checkpoints,\n",
        "    TrainingLogger  # Use unified logger\n",
        ")\n",
        "from data import get_dataloaders\n",
        "from models import create_teacher_model, create_student_model, load_teacher_model\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "EXPERIMENT: v1_baseline\n",
            "Name: Baseline Standard KD\n",
            "Description: Standard KD with basic Mixup/CutMix augmentation\n",
            "============================================================\n",
            "\n",
            "Method: standard_kd\n",
            "Temperature: 4.0\n",
            "KD Alpha: 0.7\n",
            "\n",
            "Augmentation:\n",
            "  AutoAugment: False\n",
            "  RandomErasing: False\n",
            "  Mixup: True\n",
            "  CutMix: True\n",
            "\n",
            "Training:\n",
            "  Epochs: 200\n",
            "  Batch Size: 32\n",
            "  Learning Rate: 0.001\n",
            "  Early Stopping Patience: 30\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Select Experiment\n",
        "# ============================================================\n",
        "# CHANGE THIS TO RUN DIFFERENT EXPERIMENTS\n",
        "# Options: \"v1\", \"v2\", \"v3\", \"v3.1\", \"v4\"\n",
        "# ============================================================\n",
        "\n",
        "EXPERIMENT_NAME = \"v1\"  # <-- CHANGE THIS\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "# Load configuration\n",
        "config = get_experiment(EXPERIMENT_NAME)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"EXPERIMENT: {config.experiment_id}\")\n",
        "print(f\"Name: {config.experiment_name}\")\n",
        "print(f\"Description: {config.description}\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nMethod: {config.distillation.method}\")\n",
        "print(f\"Temperature: {config.distillation.temperature}\")\n",
        "if config.distillation.method == \"dkd\":\n",
        "    print(f\"DKD Alpha: {config.distillation.dkd_alpha}\")\n",
        "    print(f\"DKD Beta: {config.distillation.dkd_beta}\")\n",
        "else:\n",
        "    print(f\"KD Alpha: {config.distillation.alpha}\")\n",
        "print(f\"\\nAugmentation:\")\n",
        "print(f\"  AutoAugment: {config.augmentation.auto_augment}\")\n",
        "print(f\"  RandomErasing: {config.augmentation.random_erasing}\")\n",
        "print(f\"  Mixup: {config.augmentation.mixup}\")\n",
        "print(f\"  CutMix: {config.augmentation.cutmix}\")\n",
        "print(f\"\\nTraining:\")\n",
        "print(f\"  Epochs: {config.base.num_epochs}\")\n",
        "print(f\"  Batch Size: {config.base.batch_size}\")\n",
        "print(f\"  Learning Rate: {config.base.learning_rate}\")\n",
        "print(f\"  Early Stopping Patience: {config.base.patience}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random seed set to: 42\n",
            "Results will be saved to: d:\\Projects\\KnowledgeDistillation\\code_v3_224\\results\\v1_baseline_20251209_024347\n",
            "Logger initialized: d:\\Projects\\KnowledgeDistillation\\code_v3_224\\results\\v1_baseline_20251209_024347\n",
            "Config saved: d:\\Projects\\KnowledgeDistillation\\code_v3_224\\results\\v1_baseline_20251209_024347\\config.json\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Set Seed and Initialize Logger\n",
        "set_seed(config.base.seed)\n",
        "\n",
        "# Get results directory with timestamp (prevents overwriting)\n",
        "# Set use_timestamp=False if you want to overwrite previous results\n",
        "USE_TIMESTAMP = True  # <-- Set to False to overwrite previous runs\n",
        "\n",
        "results_dir = config.get_results_dir(use_timestamp=USE_TIMESTAMP)\n",
        "print(f\"Results will be saved to: {results_dir}\")\n",
        "\n",
        "# Initialize Student Logger (same structure as Teacher)\n",
        "logger = TrainingLogger(config._run_id, RESULTS_DIR, model_type=\"student\")\n",
        "\n",
        "# Save configuration\n",
        "config.save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using pre-loaded 224x224 data from .pt files...\n",
            "  Note: Using num_workers=0 (required for preloaded data on Windows)\n",
            "Data loaded (preloaded):\n",
            "  Training samples: 50000\n",
            "  Test samples: 10000\n",
            "  Batch size: 32\n",
            "  Image size: 224x224\n",
            "  Augmentation: AutoAugment=False, RandomErasing=False, Mixup=True, CutMix=True\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Load Data\n",
        "# Reload data module to get latest changes\n",
        "import importlib\n",
        "import data\n",
        "importlib.reload(data)\n",
        "from data import get_dataloaders\n",
        "\n",
        "train_loader, test_loader = get_dataloaders(\n",
        "    aug_config=config.augmentation,\n",
        "    batch_size=config.base.batch_size,\n",
        "    num_workers=config.base.num_workers,\n",
        "    pin_memory=config.base.pin_memory,\n",
        "    image_size=config.base.image_size  # Pass image_size for 224x224 upscaling\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TEACHER MODEL\n",
            "============================================================\n",
            "No trained teacher found. Will train from scratch.\n",
            "Teacher Model: EfficientNetV2-L\n",
            "  Parameters: 117,362,372 (117.36M)\n",
            "  Size: 449.66 MB\n",
            "  Pretrained: True\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Create/Load Teacher Model\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TEACHER MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check for existing teacher\n",
        "teacher_path = MODELS_DIR / \"teacher_trained.pth\"\n",
        "\n",
        "if teacher_path.exists():\n",
        "    print(f\"Loading existing teacher from: {teacher_path}\")\n",
        "    teacher_model = load_teacher_model(str(teacher_path), device=device)\n",
        "    \n",
        "    # Evaluate teacher\n",
        "    teacher_results = evaluate_model(teacher_model, test_loader, device)\n",
        "    teacher_accuracy = teacher_results['accuracy']\n",
        "    print(f\"Teacher Accuracy: {teacher_accuracy:.2f}%\")\n",
        "    \n",
        "    TRAIN_TEACHER = False\n",
        "else:\n",
        "    print(\"No trained teacher found. Will train from scratch.\")\n",
        "    teacher_model = create_teacher_model(device=device)\n",
        "    teacher_accuracy = 0.0\n",
        "    TRAIN_TEACHER = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING TEACHER MODEL\n",
            "============================================================\n",
            "Checkpoints: d:\\Projects\\KnowledgeDistillation\\code_v3_224\\checkpoints\\teacher\n",
            "Logger initialized: d:\\Projects\\KnowledgeDistillation\\code_v3_224\\results\\teacher\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Teacher Epoch 1/200:  16%|█▌        | 246/1562 [11:31<1:01:38,  2.81s/it, loss=3.2354]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 73\u001b[0m\n\u001b[0;32m     69\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m teacher_model(inputs)\n\u001b[0;32m     70\u001b[0m     loss \u001b[38;5;241m=\u001b[39m lam \u001b[38;5;241m*\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(outputs, labels_a, label_smoothing\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdistillation\u001b[38;5;241m.\u001b[39mlabel_smoothing) \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m     71\u001b[0m            (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lam) \u001b[38;5;241m*\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(outputs, labels_b, label_smoothing\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdistillation\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n\u001b[1;32m---> 73\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(teacher_optimizer)\n\u001b[0;32m     75\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(teacher_model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m.\u001b[39mgrad_clip)\n",
            "File \u001b[1;32mc:\\Users\\gheith\\miniconda3\\envs\\my-gpu-env\\lib\\site-packages\\torch\\_tensor.py:629\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    621\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    622\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    627\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    628\u001b[0m     )\n\u001b[1;32m--> 629\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\gheith\\miniconda3\\envs\\my-gpu-env\\lib\\site-packages\\torch\\autograd\\__init__.py:364\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    359\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\gheith\\miniconda3\\envs\\my-gpu-env\\lib\\site-packages\\torch\\autograd\\graph.py:865\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    863\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    864\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    866\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    867\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Cell 6: Train Teacher (if needed)\n",
        "if TRAIN_TEACHER:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TRAINING TEACHER MODEL\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Teacher training configuration\n",
        "    TEACHER_EPOCHS = config.base.num_epochs\n",
        "    \n",
        "    # Create checkpoint directory for teacher\n",
        "    teacher_checkpoint_dir = CHECKPOINTS_DIR / \"teacher\"\n",
        "    teacher_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Checkpoints: {teacher_checkpoint_dir}\")\n",
        "    \n",
        "    # Initialize Teacher Logger (same structure as Student)\n",
        "    teacher_logger = TrainingLogger(\"teacher\", RESULTS_DIR, model_type=\"teacher\")\n",
        "    teacher_logger.start_training()\n",
        "    \n",
        "    # Optimizer and scheduler\n",
        "    teacher_optimizer = optim.AdamW(\n",
        "        teacher_model.parameters(),\n",
        "        lr=config.base.learning_rate,\n",
        "        weight_decay=config.base.weight_decay\n",
        "    )\n",
        "    teacher_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "        teacher_optimizer,\n",
        "        T_max=TEACHER_EPOCHS - config.base.warmup_epochs\n",
        "    )\n",
        "    \n",
        "    # Training setup\n",
        "    scaler = torch.amp.GradScaler('cuda')\n",
        "    best_teacher_acc = 0.0\n",
        "    best_teacher_weights = None\n",
        "    epochs_no_improve = 0\n",
        "    \n",
        "    for epoch in range(TEACHER_EPOCHS):\n",
        "        teacher_model.train()\n",
        "        running_loss = 0.0\n",
        "        \n",
        "        # Learning rate warmup\n",
        "        if epoch < config.base.warmup_epochs:\n",
        "            warmup_lr = config.base.learning_rate * (epoch + 1) / config.base.warmup_epochs\n",
        "            for param_group in teacher_optimizer.param_groups:\n",
        "                param_group['lr'] = warmup_lr\n",
        "        \n",
        "        loop = tqdm(train_loader, desc=f\"Teacher Epoch {epoch+1}/{TEACHER_EPOCHS}\")\n",
        "        \n",
        "        for inputs, labels in loop:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            # Apply Mixup or CutMix\n",
        "            if config.augmentation.mixup or config.augmentation.cutmix:\n",
        "                if np.random.rand() > 0.5 and config.augmentation.cutmix:\n",
        "                    inputs, labels_a, labels_b, lam = cutmix_data(\n",
        "                        inputs.clone(), labels, config.augmentation.cutmix_alpha, device\n",
        "                    )\n",
        "                elif config.augmentation.mixup:\n",
        "                    inputs, labels_a, labels_b, lam = mixup_data(\n",
        "                        inputs, labels, config.augmentation.mixup_alpha, device\n",
        "                    )\n",
        "                else:\n",
        "                    labels_a, labels_b, lam = labels, labels, 1.0\n",
        "            else:\n",
        "                labels_a, labels_b, lam = labels, labels, 1.0\n",
        "            \n",
        "            teacher_optimizer.zero_grad()\n",
        "            \n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = teacher_model(inputs)\n",
        "                loss = lam * nn.functional.cross_entropy(outputs, labels_a, label_smoothing=config.distillation.label_smoothing) + \\\n",
        "                       (1 - lam) * nn.functional.cross_entropy(outputs, labels_b, label_smoothing=config.distillation.label_smoothing)\n",
        "            \n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(teacher_optimizer)\n",
        "            nn.utils.clip_grad_norm_(teacher_model.parameters(), config.base.grad_clip)\n",
        "            scaler.step(teacher_optimizer)\n",
        "            scaler.update()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            loop.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "        \n",
        "        # Step scheduler after warmup\n",
        "        if epoch >= config.base.warmup_epochs:\n",
        "            teacher_scheduler.step()\n",
        "        \n",
        "        # Validation\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        val_results = evaluate_model(teacher_model, test_loader, device)\n",
        "        val_acc = val_results['accuracy']\n",
        "        val_loss = val_results['loss']\n",
        "        current_lr = teacher_optimizer.param_groups[0]['lr']\n",
        "        \n",
        "        # Log epoch (same as student)\n",
        "        is_best = teacher_logger.log_epoch(epoch + 1, train_loss, val_loss, val_acc, current_lr)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{TEACHER_EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | LR: {current_lr:.6f}\")\n",
        "        \n",
        "        # Save best model\n",
        "        if is_best:\n",
        "            best_teacher_acc = val_acc\n",
        "            best_teacher_weights = copy.deepcopy(teacher_model.state_dict())\n",
        "            epochs_no_improve = 0\n",
        "            print(f\"  * New best teacher! Accuracy: {best_teacher_acc:.2f}%\")\n",
        "            \n",
        "            # Save best teacher immediately\n",
        "            torch.save({\n",
        "                'model_state_dict': best_teacher_weights,\n",
        "                'accuracy': best_teacher_acc,\n",
        "                'epoch': epoch + 1\n",
        "            }, teacher_path)\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "        \n",
        "        # Save checkpoint and history every N epochs\n",
        "        if (epoch + 1) % config.base.checkpoint_frequency == 0:\n",
        "            # Save checkpoint\n",
        "            checkpoint_path = teacher_checkpoint_dir / f\"teacher_epoch_{epoch+1}.pth\"\n",
        "            torch.save({\n",
        "                'model_state_dict': teacher_model.state_dict(),\n",
        "                'optimizer_state_dict': teacher_optimizer.state_dict(),\n",
        "                'scheduler_state_dict': teacher_scheduler.state_dict(),\n",
        "                'epoch': epoch + 1,\n",
        "                'best_accuracy': best_teacher_acc\n",
        "            }, checkpoint_path)\n",
        "            print(f\"  Checkpoint saved: {checkpoint_path.name}\")\n",
        "            \n",
        "            # Save history (same as student)\n",
        "            teacher_logger.save_checkpoint_history()\n",
        "            \n",
        "            # Cleanup old checkpoints\n",
        "            cleanup_checkpoints(teacher_checkpoint_dir, keep=config.base.keep_checkpoints)\n",
        "        \n",
        "        # Early stopping\n",
        "        if epochs_no_improve >= config.base.patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "    \n",
        "    # Training complete - save final results\n",
        "    teacher_model.load_state_dict(best_teacher_weights)\n",
        "    teacher_accuracy = best_teacher_acc\n",
        "    \n",
        "    # Save final model\n",
        "    torch.save({\n",
        "        'model_state_dict': best_teacher_weights,\n",
        "        'accuracy': best_teacher_acc\n",
        "    }, teacher_path)\n",
        "    \n",
        "    # Save final results (same structure as student)\n",
        "    teacher_logger.save_final_results(\n",
        "        model_name=\"EfficientNetV2-L\",\n",
        "        total_epochs=epoch + 1,\n",
        "        early_stopped=(epochs_no_improve >= config.base.patience)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Create Student Model\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STUDENT MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "student_model = create_student_model(device=device)\n",
        "\n",
        "# Freeze teacher\n",
        "teacher_model.eval()\n",
        "for param in teacher_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Knowledge Distillation Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Setup Training\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"KNOWLEDGE DISTILLATION - {config.experiment_name}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Start timing\n",
        "logger.start_training()\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = optim.AdamW(\n",
        "    student_model.parameters(),\n",
        "    lr=config.base.learning_rate,\n",
        "    weight_decay=config.base.weight_decay\n",
        ")\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=config.base.num_epochs - config.base.warmup_epochs\n",
        ")\n",
        "\n",
        "# Training setup\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "best_acc = 0.0\n",
        "best_weights = None\n",
        "epochs_no_improve = 0\n",
        "early_stopped = False\n",
        "\n",
        "# Checkpoint directory for this experiment\n",
        "exp_checkpoint_dir = CHECKPOINTS_DIR / config.experiment_id\n",
        "exp_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\\nDistillation Method: {config.distillation.method}\")\n",
        "print(f\"Teacher Accuracy: {teacher_accuracy:.2f}%\")\n",
        "print(f\"Checkpoints: {exp_checkpoint_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Training Loop\n",
        "for epoch in range(config.base.num_epochs):\n",
        "    student_model.train()\n",
        "    running_loss = 0.0\n",
        "    valid_batches = 0\n",
        "    \n",
        "    # Learning rate warmup\n",
        "    if epoch < config.base.warmup_epochs:\n",
        "        warmup_lr = config.base.learning_rate * (epoch + 1) / config.base.warmup_epochs\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = warmup_lr\n",
        "    \n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.base.num_epochs}\")\n",
        "    \n",
        "    for inputs, labels in loop:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "        # Apply Mixup or CutMix\n",
        "        if config.augmentation.mixup or config.augmentation.cutmix:\n",
        "            if np.random.rand() > 0.5 and config.augmentation.cutmix:\n",
        "                inputs, labels_a, labels_b, lam = cutmix_data(\n",
        "                    inputs.clone(), labels, config.augmentation.cutmix_alpha, device\n",
        "                )\n",
        "            elif config.augmentation.mixup:\n",
        "                inputs, labels_a, labels_b, lam = mixup_data(\n",
        "                    inputs, labels, config.augmentation.mixup_alpha, device\n",
        "                )\n",
        "            else:\n",
        "                labels_a, labels_b, lam = labels, labels, 1.0\n",
        "        else:\n",
        "            labels_a, labels_b, lam = labels, labels, 1.0\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        with torch.amp.autocast('cuda'):\n",
        "            # Forward pass\n",
        "            student_outputs = student_model(inputs)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = teacher_model(inputs)\n",
        "            \n",
        "            # Calculate KD loss\n",
        "            loss = kd_loss_with_mixup(\n",
        "                student_outputs, teacher_outputs,\n",
        "                labels_a, labels_b, lam,\n",
        "                method=config.distillation.method,\n",
        "                temperature=config.distillation.temperature,\n",
        "                alpha=config.distillation.alpha,\n",
        "                dkd_alpha=config.distillation.dkd_alpha,\n",
        "                dkd_beta=config.distillation.dkd_beta,\n",
        "                label_smoothing=config.distillation.label_smoothing\n",
        "            )\n",
        "        \n",
        "        # Skip NaN\n",
        "        if torch.isnan(loss):\n",
        "            continue\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        nn.utils.clip_grad_norm_(student_model.parameters(), config.base.grad_clip)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        valid_batches += 1\n",
        "        loop.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "    \n",
        "    # Step scheduler after warmup\n",
        "    if epoch >= config.base.warmup_epochs:\n",
        "        scheduler.step()\n",
        "    \n",
        "    # Validation\n",
        "    train_loss = running_loss / max(valid_batches, 1)\n",
        "    val_results = evaluate_model(student_model, test_loader, device)\n",
        "    val_acc = val_results['accuracy']\n",
        "    val_loss = val_results['loss']\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    \n",
        "    # Log epoch (same structure as teacher)\n",
        "    is_best = logger.log_epoch(epoch + 1, train_loss, val_loss, val_acc, current_lr)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{config.base.num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | LR: {current_lr:.6f}\")\n",
        "    \n",
        "    # Save best model\n",
        "    if is_best:\n",
        "        best_acc = val_acc\n",
        "        best_weights = copy.deepcopy(student_model.state_dict())\n",
        "        epochs_no_improve = 0\n",
        "        print(f\"  * New best model! Accuracy: {best_acc:.2f}%\")\n",
        "        \n",
        "        # Save best model\n",
        "        torch.save(\n",
        "            {'model_state_dict': best_weights, 'accuracy': best_acc, 'epoch': epoch + 1},\n",
        "            MODELS_DIR / f\"{config.experiment_id}_best.pth\"\n",
        "        )\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "    \n",
        "    # Checkpointing and save history\n",
        "    if (epoch + 1) % config.base.checkpoint_frequency == 0:\n",
        "        checkpoint_path = exp_checkpoint_dir / f\"checkpoint_epoch_{epoch+1}.pth\"\n",
        "        save_checkpoint(\n",
        "            student_model, optimizer, scheduler,\n",
        "            epoch + 1, best_acc, logger.history,\n",
        "            checkpoint_path, is_best=(val_acc == best_acc)\n",
        "        )\n",
        "        print(f\"  Checkpoint saved: {checkpoint_path.name}\")\n",
        "        \n",
        "        # Save history (same as teacher)\n",
        "        logger.save_checkpoint_history()\n",
        "        \n",
        "        cleanup_checkpoints(exp_checkpoint_dir, keep=config.base.keep_checkpoints)\n",
        "    \n",
        "    # Early stopping\n",
        "    if epochs_no_improve >= config.base.patience:\n",
        "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
        "        early_stopped = True\n",
        "        break\n",
        "    \n",
        "    # Clear cache\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Training complete\n",
        "total_epochs = epoch + 1\n",
        "\n",
        "# Load best weights\n",
        "student_model.load_state_dict(best_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Save Final Results\n",
        "# Save training history\n",
        "logger.save_history()\n",
        "\n",
        "# Save final results (same structure as teacher)\n",
        "final_results = logger.save_final_results(\n",
        "    model_name=\"EfficientNetV2-S\",\n",
        "    total_epochs=total_epochs,\n",
        "    early_stopped=early_stopped,\n",
        "    config=config.to_dict(),\n",
        "    teacher_accuracy=teacher_accuracy\n",
        ")\n",
        "\n",
        "# Save final model\n",
        "final_model_path = MODELS_DIR / f\"{config.experiment_id}_final.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': best_weights,\n",
        "    'config': config.to_dict(),\n",
        "    'results': final_results,\n",
        "    'history': logger.history\n",
        "}, final_model_path)\n",
        "\n",
        "print(f\"\\nFinal model saved: {final_model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: Summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EXPERIMENT COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nExperiment: {config.experiment_id}\")\n",
        "print(f\"Method: {config.distillation.method}\")\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  Teacher Accuracy: {teacher_accuracy:.2f}%\")\n",
        "print(f\"  Student Accuracy: {logger.best_accuracy:.2f}%\")\n",
        "print(f\"  Retention Rate: {(logger.best_accuracy/teacher_accuracy)*100:.2f}%\")\n",
        "print(f\"\\nTraining:\")\n",
        "print(f\"  Best Epoch: {logger.best_epoch}\")\n",
        "print(f\"  Total Epochs: {total_epochs}\")\n",
        "print(f\"  Early Stopped: {early_stopped}\")\n",
        "print(f\"  Training Time: {logger.get_training_time():.1f} minutes\")\n",
        "print(f\"\\nSaved Files:\")\n",
        "print(f\"  Results: {logger.results_dir}\")\n",
        "print(f\"    - config.json\")\n",
        "print(f\"    - training_history.csv\")\n",
        "print(f\"    - training_history.json\")\n",
        "print(f\"    - final_results.json\")\n",
        "print(f\"  Model: {final_model_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "my-gpu-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
